Hadoop is an open-source software framework for storing and processing large datasets in a distributed computing environment. Developed by the Apache Software Foundation, Hadoop is designed to scale up from single servers to thousands of machines, each offering local computation and storage. It is a key technology in the field of big data analytics and [[cloud computing]].

Hadoop includes the Hadoop Distributed File System (HDFS), which allows it to store data across multiple machines, providing high aggregate bandwidth across the cluster.

One of the core components of Hadoop is the MapReduce programming model, which enables it to process large datasets in parallel across a distributed cluster. MapReduce divides the task into smaller parts, processes them in parallel, and aggregates the results.

Hadoop is highly scalable. You can start with a single machine and expand to thousands of machines. Each new machine adds more capacity to the Hadoop cluster. Hadoop is designed to handle failures at the application layer, so a high level of fault tolerance is achieved. It automatically replicates data across multiple nodes, ensuring that no data is lost if a node fails.

Hadoop can handle various forms of structured and unstructured data, giving users the flexibility to collect, process, and analyze data in ways that were not previously feasible.

Hadoop's ecosystem includes various tools and extensions to improve its functionality, including Apache Hive for [[Structured Query Language|SQL]]-like queries, Apache HBase for [[Non-relational Database|NoSQL]] data storage, Apache Pig for data processing, and Apache Spark for in-memory data processing.

Since it runs on commodity hardware and is open-source, Hadoop provides a cost-effective solution for processing large datasets. Hadoop has a large and active community of contributors and users. Major technology companies have adopted and contributed to Hadoop, making it a robust and continually evolving platform.

